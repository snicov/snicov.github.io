Now that we got a small taste, a hint of [what regression is](26.06.24.html), we can treat it more formally.

In a more general context of regression, we are assuming a certain relationship between our dependent variable $Y$ and our independent variable $X$, alternatively called regressors (that is, the variables that we are running our regression on). To be noted that the regressor. When the relationship is given by a function, we specify the function by it's parameters $\vec{\beta}$.

Therefore, the relationship can be written as $Y = f(X, \beta) + \varepsilon$ where $\varepsilon$ represents the error term of our model.

Now, this is the general setup of regression. From here, by the specific form of the function, the regression can be linear, multilinear, logistic, binomial, etc.

Without speaking about a specific form of regresison, we can still specify a general path to estimate our unknown coefficients.

As before, by our model, the dependent variable is $\tilde{Y} = f(X, \beta)$ while our true dependent variable is $Y$.

There are several ways to measure how good our model is, the most common ones are given by a loss function $L(Y, \tilde{Y})$, and the most common used loss function is the square difference $L(Y, \tilde{Y}) = \left( Y - \tilde{Y} \right)^2 = \varepsilon^2$.

The coefficients are then found to minimize the chosen total loss function, which in this case is just the sum of individual loss functions for each observation. In the case of our square difference:

$$\beta = \arg \min_{\beta} \sum_{i = 1}^N \varepsilon_i^2$$

Now let's inspect several loss functions and what are their respective advantages/disadvantages:

-   $L(Y, \tilde{Y}) = (Y - \tilde{Y})^2$ : squared loss has analytical solutions for most used types of regression, is differentiable. However, it has the tendency to be dominated by outliers. 
-   $L(Y, \tilde{Y}) = |Y - \tilde{Y}|$ : absolute loss does not have analytical solutions for linear regression and is not everywhere differentiable.
-   $L(Y, \tilde{Y}) = I(Y = \tilde{Y})$ : $0-1$ loss which is given by the indicator function.


(will be completed)

On the technical level, running a regression to find the unknown coefficients is just a calculation using the data. The assumptions are generally given by:

-   The data sample is representative of the whole data set
-   The error term have a zero expected value, conditional on the regressors. 
-   Homoskedasticity holds
-   The error terms are uncorrelated.


