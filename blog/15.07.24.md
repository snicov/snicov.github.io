Let's work out step by step the multiple linear regression. As said before, we need to specify the type of regression (multiple linear) and the loss/fit function.

We will try first the least squares loss function, also called Ordinary Least Squares (OLS) and the likelihood fit function, also called Maximum Likelihood Estimate (MLE).

Consider the model $y = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + \varepsilon$, that is, $k$ predictors. Now suppose that we have a dataset of $n$ observations:

$$y_i = \beta_0 + \beta x_{1i} + \dots + \beta_k x_{k i} + \varepsilon_i$$

Then we can represent the model in a matrix form:

$$\mathbb{y} = \mathbb{X} \mathbb{\beta}  + \mathbb{\varepsilon}$$

where $\mathbb{y}$ is a $n \times 1$ vector of the observations of the dependent variable, $\mathbb{X}$ is a $n \times (k+1)$ matrix of the observations of the predictors (with the first column a vector of ones for the intercept) and $\mathbb{\beta}$ is a $k+1 \times 1$ vector of the coefficients $\beta_0, \dots, \beta_k$ and $\mathbb{\varepsilon}$ is a $n \times 1$ vector of error terms.

Ordinary Least Squares:

We want to find such a vector $\mathbb{\beta}$ to minimize the standard euclidean norm of the vector $\mathbb{\varepsilon}$.

$$\hat{\mathbb{\beta}} = \arg \min_{\mathbb{\beta}} || \mathbb{\varepsilon} ||^2$$

$$\implies \hat{\mathbb{\beta}} = \arg \min_{\mathbb{\beta}} ||\mathbb{y} - \mathbb{X} \mathbb{\beta}||^2 = \arg \min_{\mathbb{\beta}} \left( \mathbb{y} - \mathbb{X} \mathbb{\beta}  \right)^T \left( \mathbb{y} - \mathbb{X} \mathbb{\beta}  \right)$$

Let's denote this quantity by $S(\mathbb{\beta}) = \left( \mathbb{y} - \mathbb{X} \mathbb{\beta}  \right)^T \left( \mathbb{y} - \mathbb{X} \mathbb{\beta}  \right) = \mathbb{y}^T \mathbb{y} - \mathbb{\beta}^T \mathbb{X}^T y - \mathbb{y}^T \mathbb{X} \mathbb{\beta} + \mathbb{\beta}^T \mathbb{X}^T \mathbb{X} \mathbb{\beta}$

$$\implies \frac{\partial S(\mathbb{\beta})}{\partial \mathbb{\beta}} = 0$$

If derivative with respect to a vector scares you, note that the notion of directional derivative (or derivation w.r.t. a vector) is well-defined, check [here](https://en.wikipedia.org/wiki/Directional_derivative)

Let's break the derivative term by term. First of all, $S$ is a scalar value thus we know for sure that every term in the expansion is also a scalar.

$\mathbb{y}^T \mathbb{y}$ is a scalar and thus just a constant and the derivative with respect to $\mathbb{\beta}$ is exactly zero.

$\mathbb{\beta}^T \mathbb{X}^T y$ is a scalar, and thus it is equal to its transpose $\left( \mathbb{\beta}^T \mathbb{X}^T y \right)^T = \mathbb{y}^T \mathbb{X} \mathbb{\beta}$ which is exactly the third term in the expansion, thus we will group them together as just $- 2 \mathbb{y}^T \mathbb{X} \mathbb{\beta}$ and therefore the derivative with respect to $\mathbb{\beta}$ will be $- 2 \mathbb{X}^T \mathbb{y}$

<details>

<summary>Proof</summary>

$- 2 \mathbb{y}^T \mathbb{X} \mathbb{\beta}$ as a function of $\mathbb{\beta}$ is just a dot product of the row vector $- 2 \mathbb{y}^T \mathbb{X} \mathbb{\beta}$ and the column vector $\mathbb{\beta}$ which can be alternatively rewritten as $\mathbb{a} \cdot \mathbb{\beta}$ where $\mathbb{a}$ is the column vector $\left(- 2 \mathbb{y}^T \mathbb{X} \right)^T = - 2 \mathbb{X}^T y$.

Now let's find what is the derivative:

Using the result for directional derivative:

$$\frac{\partial \mathbb{a} \cdot \mathbb{\beta}}{\partial \mathbb{\beta}} = \vec{\nabla}_{\mathbb{\beta}} \mathbb{a} \cdot \mathbb{\beta} = \vec{\nabla}_{\mathbb{\beta}} \sum_{i = 1}^{k+1} a_i \beta_i$$ which is a column vector where each component is given by $$\frac{\partial}{\partial \beta_j} \sum_{i = 1}^{k+1} a_i b_i = a_j$$ which exactly matches the definition of $\mathbb{a}$.

Therefore, the derivative with respect to $\mathbb{\beta}$ of $\mathbb{a} \cdot \mathbb{\beta}$ is $\mathbb{a}$ and, therefore, specificaly to our case:

$$\frac{\partial}{\partial \mathbb{\beta}} \left( - 2 \mathbb{y}^T \mathbb{X} \mathbb{\beta} \right) = - 2 \mathbb{X}^T y$$

------------------------------------------------------------------------

</details>

$\mathbb{\beta}^T \mathbb{X}^T \mathbb{X} \mathbb{\beta}$ is a bilinear form and thus the derivative is $2 \mathbb{X}^T \mathbb{X} \mathbb{\beta}$

<details>

<summary>Proof</summary>

$$\mathbb{\beta}^T \mathbb{X}^T \mathbb{X} \mathbb{\beta} = \sum_{i = 1}^{k + 1} \sum_{j = 1}^{k+1} \beta_i A_{ij} \beta_j$$ where $A_ij = \left( \mathbb{X}^T \mathbb{X} \right)_{ij}$ is a symmetric square matrix (since $\mathbb{A}^T = \left( \mathbb{X}^T \mathbb{X} \right)^T =  \mathbb{X}^T \mathbb{X} = \mathbb{A}$).

Therefore:

$$\frac{\partial}{\partial \beta_k} \sum_{i = 1}^{k + 1} \sum_{j = 1}^{k+1} \beta_i A_{ij} \beta_j = \sum_{i = 1}^{k + 1} \sum_{j = 1}^{k+1} \frac{\partial}{\partial \beta_k} \left( \beta_i A_{ij} \beta_j \right)  = \sum_{i = 1}^{k + 1} \sum_{j = 1}^{k+1} A_{ij} \left(\beta_i \frac{\partial \beta_j}{\partial \beta_k} + \beta_j \frac{\partial \beta_i}{\partial \beta_k}\right)$$

Let's look more at the term $\beta_i \frac{\partial \beta_j}{\partial \beta_k} + \beta_j \frac{\partial \beta_i}{\partial \beta_k}$. When $i = k$, $\frac{\partial \beta_i}{\partial \beta_k} = 1$ and $0$ otherwise. Therefore, using the Cronicker-Delta notation, the term is rewritten as:

$$\beta_i \delta_{j k} + \beta_j \delta_{i k}$$

Therefore, the $m$-th component of the derivative vector is

$$\sum_{i = 1}^{k+1} \sum_{i = 1}^{k+1} A_{ij} \left(\beta_i \delta_{j k} + \beta_j \delta_{i k} \right) = \sum_{i = 1}^{k+1} 2 A_{m i} \beta_i$$

which in vector form is

$$\frac{\partial}{\partial \mathbb{\beta}} \mathbb{\beta}^T \mathbb{X}^T \mathbb{X} \mathbb{\beta} = 2 \mathbb{X}^T \mathbb{X} \mathbb{\beta}$$

------------------------------------------------------------------------

</details>

Putting all together, we obtain that

$$\frac{\partial S(\mathbb{\beta})}{\partial \mathbb{\beta}} = - 2 \mathbb{X}^T y + 2 \mathbb{X^t} \mathbb{X} \mathbb{\beta} = 0$$

$$\implies \mathbb{X}^T \mathbb{y} = \mathbb{X}^T \mathbb{X} \mathbb{\beta}$$

$$\implies \mathbb{\beta} = \left( \mathbb{X}^T \mathbb{X} \right)^{-1} \mathbb{X}^T \mathbb{y}$$

That is, we found the OLS estimation of the parameter of the multiple linear regression.


